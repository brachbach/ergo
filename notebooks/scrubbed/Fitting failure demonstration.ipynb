{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ergo\n",
    "from ergo.scale import Scale\n",
    "import ergo.distributions as dist\n",
    "\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as np\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two convience functions for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(dist, bins=100):\n",
    "    \"\"\"Graph the pdf of some distribution\"\"\"\n",
    "    xs = np.linspace(dist.ppf(0.01), dist.ppf(0.99), bins)\n",
    "    ys = [dist.pdf(x) for x in xs]\n",
    "    plt.plot(xs, ys)\n",
    "    \n",
    "def display_params(mixture):\n",
    "    \"\"\"Print a little summary of the composition of a logistic mixture.\n",
    "    Not espeically robust, but good enough for here.\"\"\"\n",
    "    for i,c in enumerate(mixture.components):\n",
    "        if hasattr(c, \"base_dist\"):\n",
    "            c = c.base_dist\n",
    "        print(f\"Loc: {c.true_loc:3g} \\t({c.loc:3g})\",\n",
    "              f\"Scale: {c.true_s:3g} \\t({c.s:3g})\",\n",
    "              f\"Prob: {mixture.probs[i]:3g}\",\n",
    "             sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metaculus data using ought credentials\n",
    "\n",
    "def get_metaculus():\n",
    "    load_dotenv()\n",
    "    uname = str(os.getenv(\"METACULUS_USERNAME\"))\n",
    "    pwd = str(os.getenv(\"METACULUS_PASSWORD\"))\n",
    "    user_id_str = str(os.getenv(\"METACULUS_USER_ID\"))\n",
    "    if None in [uname, pwd, user_id_str]:\n",
    "        raise ValueError(\n",
    "            \".env is missing METACULUS_USERNAME, METACULUS_PASSWORD, or METACULUS_USER_ID\"\n",
    "        )\n",
    "    user_id = int(user_id_str)\n",
    "    metaculus = ergo.Metaculus(uname, pwd)\n",
    "    assert metaculus.user_id == user_id\n",
    "    return metaculus\n",
    "\n",
    "# We use the \"continious linear open question\" from the test suite\n",
    "\n",
    "metaculus = get_metaculus()\n",
    "question = metaculus.get_question(3962)\n",
    "print(\"Question Name: \", question.name)\n",
    "print(\"Question Scale: \", question.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make two test functions.  The first directly uses the LogisticMixture from_samples constructor.  The second uses the metaculus question's `get_submission_from_samples()` function, which:\n",
    "1. normalizes the samples\n",
    "2. calls the `from_samples` constructor\n",
    "3. prepares the logistic mixture, including clipping and enforcing the scale to be (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit(lm):\n",
    "    samples = np.array([lm.sample() for _ in range(0, 5000)])\n",
    "    fit = dist.LogisticMixture.from_samples(samples, fixed_params={\"num_components\": 3},\n",
    "                                        init_tries=200, opt_tries=3)\n",
    "    \n",
    "    graph(lm)\n",
    "    graph(fit)\n",
    "    \n",
    "    plt.legend([\"original ppf\", \"fit ppf\"])\n",
    "    \n",
    "    print(\"original parameters:\")\n",
    "    display_params(lm)\n",
    "    print(\"fit parameters:\")\n",
    "    display_params(fit)\n",
    "    \n",
    "    return fit\n",
    "    \n",
    "def test_question_fit(question, lm):\n",
    "    assert lm.scale == question.scale\n",
    "    \n",
    "    samples = np.array([lm.sample() for _ in range(0, 5000)])\n",
    "    fit = question.get_submission_from_samples(samples)\n",
    "        \n",
    "    normalized_lm = lm.normalize()\n",
    "    \n",
    "    graph(normalized_lm)\n",
    "    graph(fit)\n",
    "    \n",
    "    plt.legend([\"original ppf (normalized)\", \"fit ppf (normalized)\"])\n",
    "    \n",
    "    print(\"original parameters:\")\n",
    "    display_params(normalized_lm)\n",
    "    print(\"fit parameters:\")\n",
    "    display_params(fit)\n",
    "    \n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting works well in most cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_1 = dist.LogisticMixture(\n",
    "        components=[\n",
    "            dist.Logistic(loc=400000, s=100000, scale=question.scale),\n",
    "            dist.Logistic(loc=700000, s=50000, scale=question.scale),\n",
    "        ],\n",
    "        probs=[0.8, 0.2],\n",
    "    )\n",
    "\n",
    "test_fit(lm_1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It fails in certain edge cases, however -- including when there are components with very narrow distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_2 = dist.LogisticMixture(\n",
    "        components=[\n",
    "            dist.Logistic(loc=400000, s=100000, scale=question.scale),\n",
    "            dist.Logistic(loc=700000, s=5000, scale=question.scale),\n",
    "        ],\n",
    "        probs=[0.8, 0.2],\n",
    "    )\n",
    "\n",
    "test_fit(lm_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the specific bug above is due to the fact that the `logistic_mixture.from_params()` method clips the scale between 0.01 and 0.5 (when normalized).  I'm not sure why this is, but it at least seems like something that should be documented or raise a warning instead of happening implicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = test_question_fit(question, lm_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly doesn't work as well -- seemingly because of a scale issue.  As an alternative way of checking, we can take samples from the new distribution and denormalize them to compare to the original distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_samples = np.array([fit.sample() for _ in range(1000)])\n",
    "denormalized_samples = question.denormalize_samples(normalized_samples)\n",
    "\n",
    "graph(lm_1)\n",
    "seaborn.distplot(denormalized_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this problem is due to the fact that `prepare_logistic` is passed distributions which aren't truely normalized, just made to fit normalized samples -- but then assumes that they are normalized anyway."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
